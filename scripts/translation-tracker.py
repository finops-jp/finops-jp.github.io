#!/usr/bin/env python3
"""
Translation Tracker for FinOps Foundation Content

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€FinOps Foundationå…¬å¼ã‚µã‚¤ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨
ãƒ­ãƒ¼ã‚«ãƒ«ã®ç¿»è¨³æ¸ˆã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¯”è¼ƒã—ã€ç¿»è¨³çŠ¶æ…‹ã‚’è¿½è·¡ã—ã¾ã™ã€‚
"""

import os
import json
import hashlib
import re
from pathlib import Path
from typing import Dict, List, Set
import requests
from bs4 import BeautifulSoup
import yaml

# è¨­å®š
FINOPS_BASE_URL = "https://www.finops.org"
DOCS_DIR = Path("docs")
STATUS_FILE = Path(".translation-status.json")

# FinOps.orgã®ãƒ¡ã‚¤ãƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
MAIN_SECTIONS = [
    "/framework/",
    "/framework/capabilities/",
    "/framework/personas/",
    "/framework/domains/",
    "/assets/",
    "/wg/",
]


def get_page_content(url: str) -> str:
    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return ""


def extract_content_hash(html: str) -> str:
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆ"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º(ã‚µã‚¤ãƒˆæ§‹é€ ã«ä¾å­˜)
    main_content = soup.find('main') or soup.find('article') or soup.body
    
    if main_content:
        text = main_content.get_text(strip=True)
        return hashlib.sha256(text.encode()).hexdigest()
    return ""


def discover_finops_pages() -> List[Dict[str, str]]:
    """FinOps.orgã‹ã‚‰ç¿»è¨³å¯¾è±¡ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹"""
    pages = []
    
    # frameworké…ä¸‹ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    framework_url = f"{FINOPS_BASE_URL}/framework/"
    html = get_page_content(framework_url)
    
    if html:
        soup = BeautifulSoup(html, 'html.parser')
        
        # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # frameworké…ä¸‹ã®ãƒšãƒ¼ã‚¸ã®ã¿
            if href.startswith('/framework/'):
                full_url = f"{FINOPS_BASE_URL}{href}"
                
                # æ—¢ã«è¿½åŠ æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                if not any(p['url'] == full_url for p in pages):
                    # ãƒ‘ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨æ¸¬
                    path = href.strip('/').replace('/', '/') + '.md'
                    
                    pages.append({
                        'url': full_url,
                        'path': path,
                        'title': link.get_text(strip=True)
                    })
    
    return pages


def scan_local_translations() -> Dict[str, Dict]:
    """ãƒ­ãƒ¼ã‚«ãƒ«ã®ç¿»è¨³æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
    translations = {}
    
    for md_file in DOCS_DIR.rglob("*.md"):
        rel_path = md_file.relative_to(DOCS_DIR)
        
        # frontmatterã‹ã‚‰è‹±èªç‰ˆURLã‚’æŠ½å‡º
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # frontmatterã‚’æŠ½å‡º
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    # è‹±èªç‰ˆãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    match = re.search(r'\[è‹±èªç‰ˆ\]:\s*(.+)', parts[2])
                    if match:
                        source_url = match.group(1).strip()
                        
                        translations[str(rel_path)] = {
                            'source_url': source_url,
                            'local_path': str(md_file),
                            'exists': True
                        }
        except Exception as e:
            print(f"Error reading {md_file}: {e}")
    
    return translations


def load_translation_status() -> Dict:
    """ç¿»è¨³çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    if STATUS_FILE.exists():
        with open(STATUS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_translation_status(status: Dict):
    """ç¿»è¨³çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open(STATUS_FILE, 'w', encoding='utf-8') as f:
        json.dump(status, f, indent=2, ensure_ascii=False)


def generate_report(
    discovered_pages: List[Dict],
    local_translations: Dict,
    status: Dict
) -> str:
    """ç¿»è¨³çŠ¶æ…‹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    report = ["# FinOps Foundation ç¿»è¨³çŠ¶æ…‹ãƒ¬ãƒãƒ¼ãƒˆ\n"]
    report.append(f"ç”Ÿæˆæ—¥æ™‚: {__import__('datetime').datetime.now().isoformat()}\n")
    
    # çµ±è¨ˆ
    total_pages = len(discovered_pages)
    translated_pages = len(local_translations)
    untranslated_pages = total_pages - translated_pages
    
    report.append("\n## çµ±è¨ˆ\n")
    report.append(f"- ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}")
    report.append(f"- ç¿»è¨³æ¸ˆã¿: {translated_pages}")
    report.append(f"- æœªç¿»è¨³: {untranslated_pages}")
    report.append(f"- ç¿»è¨³ç‡: {translated_pages/total_pages*100:.1f}%\n")
    
    # æœªç¿»è¨³ãƒšãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
    if untranslated_pages > 0:
        report.append("\n## æœªç¿»è¨³ãƒšãƒ¼ã‚¸\n")
        
        translated_urls = {t['source_url'] for t in local_translations.values()}
        
        for page in discovered_pages:
            if page['url'] not in translated_urls:
                report.append(f"- [ ] [{page['title']}]({page['url']})")
    
    # ç¿»è¨³æ¸ˆã¿ãƒšãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
    report.append("\n## ç¿»è¨³æ¸ˆã¿ãƒšãƒ¼ã‚¸\n")
    for path, info in sorted(local_translations.items()):
        report.append(f"- [x] {path}")
        report.append(f"  - ã‚½ãƒ¼ã‚¹: {info['source_url']}")
    
    return "\n".join(report)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ” FinOps Foundationç¿»è¨³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ã®ç¿»è¨³ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    print("ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ã®ç¿»è¨³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
    local_translations = scan_local_translations()
    print(f"   {len(local_translations)}ä»¶ã®ç¿»è¨³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹")
    
    # ç¿»è¨³çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
    status = load_translation_status()
    
    # ç°¡æ˜“çš„ãªç™ºè¦‹ãƒªã‚¹ãƒˆ(å®Ÿéš›ã«ã¯ã‚‚ã£ã¨è©³ç´°ã«ã‚¹ã‚­ãƒ£ãƒ³)
    # ã“ã“ã§ã¯æ—¢å­˜ã®ç¿»è¨³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é€†ç®—
    discovered_pages = []
    for path, info in local_translations.items():
        discovered_pages.append({
            'url': info['source_url'],
            'path': path,
            'title': path
        })
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    report = generate_report(discovered_pages, local_translations, status)
    
    # çµæœã‚’å‡ºåŠ›
    print("\n" + "="*60)
    print(report)
    print("="*60)
    
    # GitHub Actionsç”¨ã®å‡ºåŠ›
    if os.getenv('GITHUB_ACTIONS'):
        with open(os.getenv('GITHUB_STEP_SUMMARY', '/dev/null'), 'a') as f:
            f.write(report)
    
    print("\nâœ… å®Œäº†!")


if __name__ == "__main__":
    main()
